---
title: "MN_Flat_File_Aggregation"
author: "Holly Kundel & Mike Verhoeven"
date: "`r Sys.Date()`"
output: html_document
---
# Preamble

libraries
```{r}
library(arrow)
library(readr)
library(dplyr)
library(stringr)
library(data.table)
library(janitor)
library(tidyr)
library(lubridate)
library(bit64)
library(ggplot2)

options(scipen = 999)
```

Update 15 Sep 2023

We recieved a new data rip from MN DNR in three pieces: effort, catch, and cpue. Why catch and CPUE you ask? Because the DNR tells me that there are historical surveys (pre ~1985) that have CPUEs but no data for the indiv fish caught in those surveys. 


# Load Data

* note depending on your local settings, file paths need correction to match local GDrive letter
* loop not breaking when last file has name issues (adjust locations of the confirmation messages)

```{r}

files_list <- list.files(path = "E:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/MN_Data/mn_raw_disaggregated_data/Archived Data", pattern = ".+\\.csv") #grabs only.csv files
files_list

n <- length(files_list)

for(i in 1:n) {
  #i = 3
  filei <- word(gsub(".csv","", files_list[i]), start = -1, sep = fixed("/"))
  #this does those two steps in one package
  assign(filei ,
          fread(paste0("E:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/MN_Data/mn_raw_disaggregated_data/",
                                          files_list[i])))
  
  # note we want to review a sorted list of column names to check misspelling etc.
  # we still need to use the columns with names like col_name_length_in, or known_units
  
  
  cde %>% # call data explainer file
    filter(`new_file_name`== filei)%>% #keep only the row relevant to this file
    select_if(~ !any(is.na(.))) %>% 
    transpose(keep.names = "newname") %>% 
    rename("oldname" = V1) %>% 
    assign("names", ., envir = .GlobalEnv)
  
  #see if any column names will not have a match! 
  # IF any pop FALSE, force stop and revist of data explainer ()
  # - e.g., named something "total catch" when actual column name was "total_catch"
  print(
    cbind(colnames(get(filei)),
          colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]
    )
  )
  
  #this line is where the warnings are coming from
  # break the loop if the current file has column names not in the data explainer
  if (all(cbind(colnames(get(filei)),  colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ])[,2]) == FALSE ) break
  
  # append old col names into new "notes" columns:
  get(filei)[ , (names[ str_detect(newname, "notes") , oldname   ,  ]) := Map(paste, colnames(.SD), .SD, sep = ':') , .SDcols =  names[ str_detect(newname, "notes") , oldname   ,  ] ]
  
  #now rename that file's colnames
  setnames(get(filei), colnames(get(filei)), names[!str_detect(newname,"unique_row_key")] [match(names(get(filei)),names[!str_detect(newname,"unique_row_key"),oldname]), newname] )
  
  #append all other data from data explainer
  unusedbits <- 
    data.table(
      matrix(
        rep(names[ !newname %in% colnames(get(filei)) , oldname , ],
            each = nrow(get(filei))
        ),
        nrow = nrow(get(filei)),
        dimnames = list(rep(NA,nrow(get(filei))),
                        names[ !newname %in% colnames(get(filei)) , newname , ])
        )
      )
  
  #add all not yet used columns from data explainer:
  get(filei)[ , (names[ !newname %in% colnames(get(filei)) , newname , ]) := unusedbits[] ]

  #confirm import of files:  
  print(paste(filei ,"added to workspace" ))  
  
} 

#generate a file list to import
files_list <- list.files(path = "E:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/MN_Data/mn_raw_disaggregated_data", pattern = ".+\\.csv") #grabs only.csv files
files_list

#object for use in loop (simple length of file list)
n <- length(files_list)

for(i in 1:n) {
  #i = 3
  filei <- word(gsub(".csv","", files_list[i]), start = -1, sep = fixed("/"))
  #this does those two steps in one package
  assign(filei ,
          fread(paste0("E:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/MN_Data/mn_raw_disaggregated_data/",
                                          files_list[i])))
  
  # if the file is a crosswalk, do not rename anything, just loop to the confirm import line
  if(str_detect(filei, "crosswalk")) {  #confirm import of files:  
    print(paste(filei ,"added to workspace" ))  
    #confirm import of files:  
    print(paste(i ,"files added to workspace" )) ; next}
  
  
  # note we want to review a sorted list of column names to check misspelling etc.
  # we still need to use the columns with names like col_name_length_in, or known_units
  
  
  cde %>% # call data explainer file
    filter(`new_file_name`== filei)%>% #keep only the row relevant to this file
    select_if(~ !any(is.na(.))) %>% 
    transpose(keep.names = "newname") %>% 
    rename("oldname" = V1) %>% 
    assign("names", ., envir = .GlobalEnv)
  
  #see if any column names will not have a match! 
  # IF any pop FALSE, force stop and revist of data explainer ()
  # - e.g., named something "total catch" when actual column name was "total_catch"
  print(
    cbind(colnames(get(filei)),
          colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]
    )
  )
  
  
  # break the loop if the current file has column names not in the data explainer
  # if (all(cbind(colnames(get(filei)),  colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ])[,2]) == FALSE ) break
  if (all(colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]) == FALSE ) break
  
  
  # append old col names into new "notes" columns:
  get(filei)[ , (names[ str_detect(newname, "notes") , oldname   ,  ]) := Map(paste, colnames(.SD), .SD, sep = ':') , .SDcols =  names[ str_detect(newname, "notes") , oldname   ,  ] ]
  
  #now rename that file's colnames
  setnames(get(filei), colnames(get(filei)), names[!str_detect(newname,"unique_row_key")] [match(names(get(filei)),names[!str_detect(newname,"unique_row_key"),oldname]), newname] )
  
  #append all other data from data explainer
  unusedbits <- 
    data.table(
      matrix(
        rep(names[ !newname %in% colnames(get(filei)) , oldname , ],
            each = nrow(get(filei))
        ),
        nrow = nrow(get(filei)),
        dimnames = list(rep(NA,nrow(get(filei))),
                        names[ !newname %in% colnames(get(filei)) , newname , ])
        )
      )
  
  #add all not yet used columns from data explainer:
  get(filei)[ , (names[ !newname %in% colnames(get(filei)) , newname , ]) := unusedbits[] ]

  rm(names, unusedbits)
  
  #confirm import of files:  
  print(paste(filei ,"added to workspace" ))  
  #confirm import of files:  
  print(paste(i ,"files added to workspace" )) 

  
} 
  #confirm import of files:  
  print(paste(i ,"files added to workspace" ))
  #confirm import of files:  
  print(paste(n-i ,"remaining to be added" )) 

```


# Effort

```{r}

# effort dataset has a unique key in the combo of (lake_id, date.2(survey date), gear, survey type)
mn_effort_18aug2023[          , .N , .(lake_id, date.1, sampling_method, survey_type.1) ]

#note that there is a component count column in here that seems like it's related to effort, but not exactly clear to me:
mn_effort_18aug2023[ , summary(as.numeric(gsub("COMPONENT_COUNT:","",gear_data_notes.2))) ,]
mn_effort_18aug2023[ , hist(as.numeric(gsub("COMPONENT_COUNT:","",gear_data_notes.2)), breaks = 100) ,]


# we have two total effort cols, one is EF seconds, which I think is redundant to the main effort column:
mn_effort_18aug2023[ , total_effort_2 := gsub("EF_SECONDS:", "" , total_effort_2), ]
mn_effort_18aug2023[ total_effort_2 == "NA" , total_effort_2 := NA  ,]
mn_effort_18aug2023[  , total_effort_2 := as.numeric(total_effort_2) ,]


mn_effort_18aug2023[ , .N , .(is.na(total_effort_2), is.na(total_effort_1.1))] # any cases where all dat in total.effort.2

#here we can see that ef time is essentially covered by the total effeort 1 column (and probably more complete in that column)
plot(mn_effort_18aug2023[ !is.na(total_effort_2) , total_effort_2,]/3600~ mn_effort_18aug2023[ !is.na(total_effort_2) , total_effort_1.1 ,])
#what about the one's where they don't jive?
mn_effort_18aug2023[ round(total_effort_2/3600, 2) != total_effort_1.1, plot(total_effort_2/3600~total_effort_1.1), ]
abline(0,1)

#we don't need that extra total eff column
mn_effort_18aug2023[ , total_effort_2 := NULL , ]

#the DNR sent along two values for counts, one where they count only cases where CPUE was marked yes in the indiv fish data and one count including only where cpue was marked no in the indiv fish data

mn_effort_18aug2023[total_count.2>0 , .(total_count.1,total_count.2) , ]
mn_effort_18aug2023[ , plot(total_count.1~total_count.2) , ]

#are there any effort data with NO associated count (i.e., nothing caught)? 
mn_effort_18aug2023[   , .N , .(total_count.1 == 0, total_count.2 == 0) ]
#NOPE

#Thus, because these data are gear level, this is agg'd across species so relatively useless.

#drop these columns
mn_effort_18aug2023[ , `:=` ("total_count.1" = NULL,"total_count.2" = NULL) , ]


#fix up dates
mn_effort_18aug2023[ , date_clean :=  as.IDate(word(date.1, 1, sep = fixed(" ")), format = "%m/%d/%Y"),]
mn_effort_18aug2023[ is.na(date_clean), ]

mn_effort_18aug2023[ , hist(yday(date_clean)) ,]
mn_effort_18aug2023[ , hist(year(date_clean)) ,]

#check other fields
names(mn_effort_18aug2023)


#tidy sampling methods
mn_effort_18aug2023[ , .N , sampling_method ]
mn_effort_18aug2023[ , .N , .(sampling_method, sampling_method_abbrev)]

#keep the abbreviation/code column because that's the only thing in the CPUE file
mn_effort_18aug2023[ , .N , sampling_method_abbrev , ]
mn_cpue_21aug2023[ , .N , sampling_method_abbrev]

any(is.na(match(mn_effort_18aug2023[ , unique(sampling_method_abbrev),], mn_cpue_21aug2023[ , unique(sampling_method_abbrev) , ])))
#all abbreviations are covered (we could come back to that to add full gears to the cpues)

sort(names(mn_effort_18aug2023))

#year
mn_effort_18aug2023[ , unique(year) , ]
mn_effort_18aug2023[ is.na(year),]
 sum(mn_effort_18aug2023[ ,year != year(date_clean)])
#non-issue, delete year column
 
mn_effort_18aug2023[ , year := NULL] 
 

#effort units
mn_effort_18aug2023[ , .N , sampling_method ]


mn_effort_18aug2023[ , summary(total_effort_1.1) ,]

mn_effort_18aug2023[ total_effort_1.1 <0 , , ]

ggplot( mn_effort_18aug2023[!total_effort_1.1 < 0 ], aes(total_effort_1.1, group = sampling_method))+
  geom_density()+
  facet_wrap(~sampling_method, scales = "free")

# we can get units for these effort data from here (p45ish):
# https://files.dnr.state.mn.us/publications/fisheries/special_reports/180.pdf




```



# CPUE 2(3) ways
We have CPUE directly form the DNR, we can generate our own cpue using the individual fish data (both including and excluding the fish marked for CPUE inclusion)

```{r}

  #cpue data
  mn_cpue_21aug2023[  , .N , .(lake_id, date.1, sampling_method_abbrev, survey_type.1, species.1) ]
  sum(duplicated(mn_cpue_21aug2023))
  #not really sure why there would be duplicated cpues in there... 
  mn_cpue_21aug2023[duplicated(mn_cpue_21aug2023) , .N, .(lake_name.1, lake_id, date.1) ]
  #if these cpues are calc'd per net or instance, that could cause it. how many of each gear in each survey?
  mn_cpue_21aug2023[ , .N, .(lake_name.1, lake_id, date.1, species.1, sampling_method_abbrev) ][, summary(N)]
  mn_cpue_21aug2023[ ,mean(total_effort_1.1), .(sampling_method_abbrev) ]
  mn_cpue_21aug2023[ , .N, .(lake_name.1, lake_id, date.1, species.1, sampling_method_abbrev) ][N>1, ]

  #not likely a problem. I think sometimes multiple rows are reported for a given gear on a survey, when the catch is == between two of them you get an aparrent "duplicate"
  
  #we'll be circling back to this dataset as a way to get the historical un-lengthed fish data into our fish-obs-as-rows format
  
  
  #fix up dates
  mn_cpue_21aug2023[ , date_clean :=  as.IDate(date.1),]
  mn_cpue_21aug2023[ is.na(date_clean), ]

  mn_cpue_21aug2023[ , hist(yday(date_clean)) ,]
  mn_cpue_21aug2023[ , hist(year(date_clean)) ,]
  
  
  #check overlap betweek effort and CPUE:
  mn_cpue_21aug2023[            , .N , .(lake_id, date.1, sampling_method_abbrev, survey_type.1) ]
  mn_effort_18aug2023[          , .N , .(lake_id, date.1, sampling_method_abbrev, survey_type.1) ]

   mn_cpue_21aug2023[            , .N , .(lake_id, date_clean, sampling_method_abbrev, survey_type.1) ][
     mn_effort_18aug2023[          , .N , .(lake_id, date_clean, sampling_method_abbrev, survey_type.1) ], 
     on = .(lake_id, date_clean, sampling_method_abbrev, survey_type.1) , 
   ]
  
  # Awesome-- the effort data are entirely contained in the cpue file. So here's a plan: 
   # 1. we use the fish obs and effort to set up our obs level file
   #  2a. Identify CPUE rows unmatched in the obs level surveys (for which we have nothing in effort or indiv fish). 
   #  2b. expand those rows into obs level format (but no wieghts or lengths)
   #  2c. bring them back to that main obs level file
  
```


# Prep & Scope Indiv Fish Data

```{r}

#inspect data
colnames(mn_indiv_fish_23aug2023)
sort(colnames(mn_indiv_fish_23aug2023))

setcolorder(mn_indiv_fish_23aug2023, c("lake_id", "date.2", "date.1", "sampling_method_abbrev", "survey_type.1")) 

mn_indiv_fish_23aug2023[ , .N , .(sampling_method_abbrev, species.1)  ]
mn_indiv_fish_23aug2023[ species.1 == "WAE", .N , .(sampling_method_abbrev)  ] 
mn_indiv_fish_23aug2023[ species.1 == "WAE", .N , .(sampling_method_abbrev)  ][ , sum(N) , ]

#to-do:
# make these clean: "lake_id", "date_clean", "sampling_method", "survey_type.1"

#dates (from data explainer we know that date.2 is survey date, we prob want to join on that one to connect to the effort data)
mn_indiv_fish_23aug2023[ , .(date.2, date.1), ]
mn_indiv_fish_23aug2023[ , .N , .(date1=is.na(date.1), date2=is.na(date.2)) ]

#change dates to Idate:
mn_indiv_fish_23aug2023[ , date_clean := as.IDate(date.2, format = "%m/%d/%Y") ,]

mn_indiv_fish_23aug2023[ , summary(date_clean) , ]
mn_indiv_fish_23aug2023[is.na(date_clean) , .N ]
#no missingness!


#lake_id
mn_indiv_fish_23aug2023[ , summary(lake_id) ,]
mn_indiv_fish_23aug2023[ , str(lake_id), ]
mn_effort_18aug2023[ , str(lake_id) ,]
#no issues here


#tidy sampling methods
mn_indiv_fish_23aug2023[ , .N , .(sampling_method_abbrev)]
mn_effort_18aug2023[ , .N , .(sampling_method, sampling_method_abbrev)]


any(is.na(match(mn_effort_18aug2023[ , unique(sampling_method_abbrev),], mn_indiv_fish_23aug2023[ , unique(sampling_method_abbrev) , ])))
#all abbreviations are covered (we could come back to that to add full gears to the indiv fish, but that'll hapen in the merge, so skipped here. )



#survey.type
mn_indiv_fish_23aug2023[ , .N , survey_type.1 ]
mn_effort_18aug2023[ , .N , survey_type.1]

mn_indiv_fish_23aug2023[ , .N , survey_type.1 ][ ,survey_type.1 , ]


# generate a type table (these codes can be found in MN_Data folder)
survey_type <- data.table(code = mn_indiv_fish_23aug2023[ , .N , survey_type.1 ][ ,survey_type.1],
                          fullname = mn_effort_18aug2023[ , .N , survey_type.1][ ,survey_type.1 ][c(6,2,4,5,8,7,13,9,1,11,10,3,12,14) ]
)

#save to file
# fwrite(survey_type, file = "Data_and_Scripts\\Data\\output\\mn_survey_type_code_key.csv")

#execute
any(is.na(match(mn_indiv_fish_23aug2023[ , survey_type.1], survey_type[,code])))
survey_type[match(mn_indiv_fish_23aug2023[ , survey_type.1], survey_type[,code]), fullname]

mn_indiv_fish_23aug2023[ , survey_type.1 := survey_type[match(mn_indiv_fish_23aug2023[ , survey_type.1], survey_type[,code]), fullname]   ,]
rm(survey_type)

#uncount indiv_fish
mn_indiv_fish_23aug2023[, summary(total_count.1) , ]
mn_indiv_fish_23aug2023[total_count.1 == 0 , .N , ]

#expected result size:
mn_indiv_fish_23aug2023[ , sum(total_count.1) , ] + mn_indiv_fish_23aug2023[total_count.1 == 0 , .N , ]


#execute dataset expansion/uncount
mn_indiv_fish_23aug2023 <- 
rbind(
  mn_indiv_fish_23aug2023[total_count.1 == 0 , , ], 
  uncount(mn_indiv_fish_23aug2023, weights = total_count.1, .remove = F)
)



#summarize ages
mn_indiv_fish_23aug2023[ , .N , age ]
mn_indiv_fish_23aug2023[ , .N , .(age=!is.na(age))]


```



# Merge effort and indiv fish

```{r}
#merge the effort and indiv fish data


#scope the merge:
#99% of the wb in the effort data are represneted in indiv fish data
sum(mn_indiv_fish_23aug2023[ , unique(lake_id) , ] %in% mn_effort_18aug2023[ , unique(lake_id)])/mn_effort_18aug2023[, length(unique(lake_id)) ,] 

#100% of the wb in the indiv fish data are represneted in  effort data
sum(mn_indiv_fish_23aug2023[ , unique(lake_id) , ] %in% mn_effort_18aug2023[ , unique(lake_id)])/mn_indiv_fish_23aug2023[, length(unique(lake_id)) ,] 

#merge the effort and indiv fish data

intersect(names(mn_effort_18aug2023), names(mn_indiv_fish_23aug2023))


#we know that dates are a problem, can we schmooze that join?
#we leave many column names out of this mereg to avoid disconnecting things where one dataset has empty info
flatfish <- merge(mn_effort_18aug2023,mn_indiv_fish_23aug2023, by = c("lake_id", "date_clean" , "sampling_method_abbrev", "survey_type.1"), suffixes = c("_effort", "_indivfish"), all = T)

flatfish[ , .N , sampling_method]
flatfish[ , .N, species.1]

flatfish[ (species.1 == "WAE" & sampling_method %in% c("Standard gill net sets")), .N, .(year = year(date_clean), age = !is.na(age), length = !is.na(length.1))][order(year, -N)]


#this chunk is what Mike thinks GH wants for today
names(flatfish)

#how many unique surveys exist for each category of has effort and has indiv fish data?
flatfish[ , length(unique(paste(lake_id, date_clean , sampling_method, survey_type.1))) , .(effort = !is.na(state_effort), indiv_fish_dat = !is.na(state_indivfish)) ]
  
#sum of all
  flatfish[ , length(unique(paste(lake_id, date_clean , sampling_method, survey_type.1))) , .(effort = !is.na(state_effort), indiv_fish_dat = !is.na(state_indivfish)) ][ , sum(V1) , ]

rm(mn_indiv_fish_23aug2023,mn_effort_18aug2023)
  
  
#now roll the catch CPUE data back to this bigger file:

   mn_cpue_21aug2023[ 
     flatfish[          , .N , .(lake_id, date_clean, sampling_method_abbrev, survey_type.1) ], 
     on = .(lake_id, date_clean, sampling_method_abbrev, survey_type.1) , 
   ]
  
  mn_cpue_21aug2023[is.na(surveyXgear_indfish) , .N]
  
  #trim to only the not already covered data:
  mn_cpue_21aug2023 <- mn_cpue_21aug2023[is.na(surveyXgear_indfish) , ]
  
  mn_cpue_21aug2023[ , summary(total_count.1) , ]
  
  #execute dataset expansion/uncount
mn_cpue_21aug2023 <- 
rbind(
  mn_cpue_21aug2023[total_count.1 == 0 , , ], 
  uncount(mn_cpue_21aug2023, weights = total_count.1, .remove = F)
)

names(flatfish)


  #now bring that together with the flatfish data:
  flatfish <- rbindlist(list(flatfish, mn_cpue_21aug2023 ),
                               fill = TRUE,
                               use.names = TRUE)
  
  
  
  #for each of those with both effort and fish data, summarize catch by species (bring effort along too!)   
flatfish[ !is.na(state_effort)&!is.na(state_indivfish) , .N , .(lake_id, date_clean , sampling_method, survey_type.1, species.1, total_effort_1.1, effort_units.1) ]





  ## SIDEBAR! Flagging suspected issues
  #summarize the total effort for ef data
  flatfish[ effort_units.1 == "knownunit_hours" , hist(total_effort_1.1, breaks = 100) , ]# there are some very wierd values in here! Def want to flag these for review by a user! Here's an example of that:
  flatfish[ effort_units.1 == "knownunit_hours" , summary(total_effort_1.1) , ]
  flatfish[ effort_units.1 == "knownunit_hours" & !is.na(total_effort_1.1) , .(mean(total_effort_1.1) , sd(total_effort_1.1)) , ] #find sd and mean
  flatfish[ effort_units.1 == "knownunit_hours" & total_effort_1.1 > 1.464117 + (5*flatfish[ effort_units.1 == "knownunit_hours" & !is.na(total_effort_1.1) , sd(total_effort_1.1) , ])   , flagged_effort := TRUE , ] #flag records > 5 sd from mean
  flatfish[ , .N , flagged_effort]
  
  
#cast summarized catch data wide:
wide_complete <- dcast(flatfish[ !is.na(state_effort)&!is.na(state_indivfish) , .N , .(lake_id, date_clean , sampling_method, survey_type.1, species.1, total_effort_1.1, effort_units.1) ], ... ~ species.1 , value.var = "N", fill = 0)

# calc cpue for all wae
wide_complete[ , .(lake_id, date_clean , sampling_method, survey_type.1, total_effort_1.1, effort_units.1, WAE) ,]
wide_complete[ , wae_cpue := WAE/total_effort_1.1 , ]
# check mean cpue for gillnets
wide_complete[ str_detect(sampling_method, "gill net") , mean(wae_cpue) , sampling_method ]

  
flatfish[ species.1 == "WAE" , .N ,]

#where lake Id is a kittle no, we can drop those data (lake_id blank, lake_id_chr has kittle)
flatfish[ , .N , .(lake_id, lake_id_chr, date_clean , sampling_method, survey_type.1, total_effort_1.1, species.1)] 
  flatfish[!is.na(lake_id), .N , .(lake_id, lake_id_chr, date_clean , sampling_method, survey_type.1, total_effort_1.1, species.1)]
  flatfish <- flatfish[!is.na(lake_id)]

#lets get a cpue for walleye
  flatfish[ , , .(lake_id, date_clean , sampling_method, survey_type.1) ][]
    
  
flatfish[ , summary(date_clean) ,]

flatfish[ , hist(year(date_clean)) , ]

#walleye data product:
wae_complete_surveys <- wide_complete[ , .(lake_id, date_clean , sampling_method, survey_type.1, total_effort_1.1, effort_units.1, WAE) ,]
  
wae_records <- flatfish[ species.1 == "WAE" , ,] 

wae <- merge(wae_complete_surveys, wae_records, all = T, by = c("lake_id", "date_clean" , "sampling_method", "survey_type.1", "total_effort_1.1", "effort_units.1"))

wae[ , .N , .(lake_id, date_clean , sampling_method, survey_type.1, total_effort_1.1, effort_units.1, WAE) ] 


wae_gn_records <- wae[ str_detect(sampling_method, "gill nets") , , ]





#save to disk:

# saveRDS(mn_cpue_21aug2023, file = "Data_and_Scripts\\Data\\output\\mn_hist_data_flat.rds")
# saveRDS(flatfish, file = "Data_and_Scripts\\Data\\output\\mn_flat_effort_indivfish_merge.rds")


# saveRDS(DNR_calcd_catch, file = "Data_and_Scripts\\Data\\output\\mn_dnr_calcd_catch.rds")
# saveRDS(indiv_fish, file = "Data_and_Scripts\\Data\\output\\mn_indivfish.rds")
# saveRDS(wae_gn_records, file = "Data_and_Scripts\\Data\\output\\mn_wae_gn.rds")


```

# Save Parquet

```{r}




flatfish <- write_dataset(dataset = flatfish, path = "Data_and_Scripts/Data/output/mn_file_arrow")

mn_data <- open_dataset("Data_and_Scripts/Data/output/mn_file_arrow/")

glimpse(mn_data)

# gear full names need filling out
mn_data %>% 
  group_by(sampling_method_abbrev, sampling_method) %>% 
  summarize(n = n()) %>% 
  collect() %>% 
  print(n = nrow(.))

gear_table_mn <- mn_data %>% 
  filter(!is.na(sampling_method)) %>%
  select( sampling_method_abbrev, sampling_method) %>% 
  unique() %>% 
  compute() %>% 
  collect() %>% 
  print(n = nrow(.))

mn_data %>% 
  mutate( sampling_method = )

mn_data %>% 
  select(!sampling_method) %>% 
  inner_join(. , gear_table_mn, by = "sampling_method_abbrev") %>% 
  group_by(sampling_method_abbrev, sampling_method) %>% 
  summarize(n = n()) %>% 
  collect() %>% 
  print(n = nrow(.))


mn_data %>% 
  filter(species.1 == "WAE") %>% 
  group_by(sampling_method, sampling_method_abbrev) %>% 
  summarise(n = n()) %>% 
  arrange( n) %>% 
  collect() %>% 
  print(n = nrow(.))


```


#Diversity exploration
```{r}

library(vegan)

colnames(wide_complete)
wide_complete[ , 7:112 ,]

diversity(wide_complete[ , 7:112 ,], index = "invsimpson" )


wide_complete[ , invsimpson := diversity(wide_complete[ , 7:112 ,], index = "invsimpson" ) ,]

wide_complete[ , alltime_masIS:= round(max(invsimpson),0)  ,  .(lake_id, sampling_method)]
wide_complete[ , alltime_medIS:= median(invsimpson)  ,  .(lake_id, sampling_method)]



ggplot(wide_complete[sampling_method %in% c("Standard gill nets", "Standard trap nets")] , aes(year(date_clean), invsimpson)  )+
  geom_line(aes(group = lake_id))+
  facet_grid(alltime_masIS~sampling_method)+
  geom_smooth()


ggplot(data = wide_complete[ , .(var(invsimpson) , alltime_medIS), lake_id ], aes(alltime_medIS, V1))+
  geom_point()+geom_smooth()+ ylab("variance in Diversity") + xlab("alltime median Diversity")

ggplot(data = wide_complete[ , .(var(WAE/total_effort_1.1) , alltime_medIS), lake_id ], aes(alltime_medIS, V1))+
  geom_point()+geom_smooth()+ ylab("variance in walleye cpue") + xlab("alltime median Diversity")+scale_y_log10()







```
