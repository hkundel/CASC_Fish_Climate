---
title: "MN_Flat_File_Aggregation"
author: "Holly Kundel & Mike Verhoeven"
date: "`r Sys.Date()`"
output: html_document
---
# Preamble

libraries
```{r}
library(arrow)
library(readr)
library(dplyr)
library(stringr)
library(data.table)
library(janitor)
library(tidyr)
library(lubridate)
library(bit64)
library(ggplot2)

options(scipen = 999)
```

Update 15 Sep 2023

We recieved a new data rip from MN DNR in three pieces: effort, catch, and cpue. Why catch and CPUE you ask? Because the DNR tells me that there are historical surveys (pre ~1985) that have CPUEs but no data for the indiv fish caught in those surveys. 


# Load Data

* note depending on your local settings, file paths need correction to match local GDrive letter
```{r}

files_list <- list.files(path = "E:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/MN_Data/mn_raw_disaggregated_data", pattern = ".+\\.csv") #grabs only.csv files
files_list

n <- length(files_list)

for(i in 1:n) {
  #i = 3
  filei <- word(gsub(".csv","", files_list[i]), start = -1, sep = fixed("/"))
  #this does those two steps in one package
  assign(filei ,
          fread(paste0("E:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/MN_Data/mn_raw_disaggregated_data/",
                                          files_list[i])))
  
  # note we want to review a sorted list of column names to check misspelling etc.
  # we still need to use the columns with names like col_name_length_in, or known_units
  
  
  cde %>% # call data explainer file
    filter(`new_file_name`== filei)%>% #keep only the row relevant to this file
    select_if(~ !any(is.na(.))) %>% 
    transpose(keep.names = "newname") %>% 
    rename("oldname" = V1) %>% 
    assign("names", ., envir = .GlobalEnv)
  
  #see if any column names will not have a match! 
  # IF any pop FALSE, force stop and revist of data explainer ()
  # - e.g., named something "total catch" when actual column name was "total_catch"
  print(
    cbind(colnames(get(filei)),
          colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]
    )
  )
  
  #this line is where the warnings are coming from
  # break the loop if the current file has column names not in the data explainer
  if (all(cbind(colnames(get(filei)),  colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ])[,2]) == FALSE ) break
  
  # append old col names into new "notes" columns:
  get(filei)[ , (names[ str_detect(newname, "notes") , oldname   ,  ]) := Map(paste, colnames(.SD), .SD, sep = ':') , .SDcols =  names[ str_detect(newname, "notes") , oldname   ,  ] ]
  
  #now rename that file's colnames
  setnames(get(filei), colnames(get(filei)), names[!str_detect(newname,"unique_row_key")] [match(names(get(filei)),names[!str_detect(newname,"unique_row_key"),oldname]), newname] )
  
  #append all other data from data explainer
  unusedbits <- 
    data.table(
      matrix(
        rep(names[ !newname %in% colnames(get(filei)) , oldname , ],
            each = nrow(get(filei))
        ),
        nrow = nrow(get(filei)),
        dimnames = list(rep(NA,nrow(get(filei))),
                        names[ !newname %in% colnames(get(filei)) , newname , ])
        )
      )
  
  #add all not yet used columns from data explainer:
  get(filei)[ , (names[ !newname %in% colnames(get(filei)) , newname , ]) := unusedbits[] ]

  #confirm import of files:  
  print(paste(filei ,"added to workspace" ))  
  
} 

#generate a file list to import
files_list <- list.files(path = "E:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/MN_Data/mn_raw_disaggregated_data", pattern = ".+\\.csv") #grabs only.csv files
files_list

#object for use in loop (simple length of file list)
n <- length(files_list)

for(i in 1:n) {
  #i = 3
  filei <- word(gsub(".csv","", files_list[i]), start = -1, sep = fixed("/"))
  #this does those two steps in one package
  assign(filei ,
          fread(paste0("E:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/MN_Data/mn_raw_disaggregated_data/",
                                          files_list[i])))
  
  # if the file is a crosswalk, do not rename anything, just loop to the confirm import line
  if(str_detect(filei, "crosswalk")) {  #confirm import of files:  
    print(paste(filei ,"added to workspace" ))  
    #confirm import of files:  
    print(paste(i ,"files added to workspace" )) ; next}
  
  
  # note we want to review a sorted list of column names to check misspelling etc.
  # we still need to use the columns with names like col_name_length_in, or known_units
  
  
  cde %>% # call data explainer file
    filter(`new_file_name`== filei)%>% #keep only the row relevant to this file
    select_if(~ !any(is.na(.))) %>% 
    transpose(keep.names = "newname") %>% 
    rename("oldname" = V1) %>% 
    assign("names", ., envir = .GlobalEnv)
  
  #see if any column names will not have a match! 
  # IF any pop FALSE, force stop and revist of data explainer ()
  # - e.g., named something "total catch" when actual column name was "total_catch"
  print(
    cbind(colnames(get(filei)),
          colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]
    )
  )
  
  
  # break the loop if the current file has column names not in the data explainer
  # if (all(cbind(colnames(get(filei)),  colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ])[,2]) == FALSE ) break
  if (all(colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]) == FALSE ) break
  
  
  # append old col names into new "notes" columns:
  get(filei)[ , (names[ str_detect(newname, "notes") , oldname   ,  ]) := Map(paste, colnames(.SD), .SD, sep = ':') , .SDcols =  names[ str_detect(newname, "notes") , oldname   ,  ] ]
  
  #now rename that file's colnames
  setnames(get(filei), colnames(get(filei)), names[!str_detect(newname,"unique_row_key")] [match(names(get(filei)),names[!str_detect(newname,"unique_row_key"),oldname]), newname] )
  
  #append all other data from data explainer
  unusedbits <- 
    data.table(
      matrix(
        rep(names[ !newname %in% colnames(get(filei)) , oldname , ],
            each = nrow(get(filei))
        ),
        nrow = nrow(get(filei)),
        dimnames = list(rep(NA,nrow(get(filei))),
                        names[ !newname %in% colnames(get(filei)) , newname , ])
        )
      )
  
  #add all not yet used columns from data explainer:
  get(filei)[ , (names[ !newname %in% colnames(get(filei)) , newname , ]) := unusedbits[] ]

  #confirm import of files:  
  print(paste(filei ,"added to workspace" ))  
  #confirm import of files:  
  print(paste(i ,"files added to workspace" )) 

  
} 
  #confirm import of files:  
  print(paste(i ,"files added to workspace" ))
  #confirm import of files:  
  print(paste(n-i ,"remaining to be added" )) 

```


# Effort

```{r}

# effort dataset has a unique key in the combo of (lake_id, date.2(survey date), gear, survey type)
mn_effort_18aug2023[          , .N , .(lake_id, date.1, sampling_method, survey_type.1) ]

#note that there is a component count column in here that seems like it's related to effort, but not exactly clear to me:
mn_effort_18aug2023[ , summary(as.numeric(gsub("COMPONENT_COUNT:","",gear_data_notes.2))) ,]
mn_effort_18aug2023[ , hist(as.numeric(gsub("COMPONENT_COUNT:","",gear_data_notes.2)), breaks = 100) ,]


# we have two total effort cols, one is EF seconds, which I think is redundant to the main effort column:
mn_effort_18aug2023[ , .N , .(is.na(total_effort_2), is.na(total_effort_1.1))] # any cases where all dat in total.effort.2

#here we can see that ef time is essentially covered by the total effeort 1 column (and probably more complete in that column)
plot(mn_effort_18aug2023[ !is.na(total_effort_2) , total_effort_2/3600 ,]~ mn_effort_18aug2023[ !is.na(total_effort_2) , total_effort_1.1 ,])
#what about the one's where they don't jive?
mn_effort_18aug2023[ round(total_effort_2/3600, 2) != total_effort_1.1, plot(total_effort_2/3600~total_effort_1.1), ]
abline(0,1)

#we don't need that extra total eff column
mn_effort_18aug2023[ , total_effort_2 := NULL , ]

#the DNR sent along two values for counts, one where they count only cases where CPUE was marked yes in the indiv fish data and one count including only where cpue was marked no in the indiv fish data

mn_effort_18aug2023[total_count.2>0 , .(total_count.1,total_count.2) , ]
mn_effort_18aug2023[ , plot(total_count.1~total_count.2) , ]

#are there any effort data with NO associated count (i.e., nothing caught)? 
mn_effort_18aug2023[   , .N , .(total_count.1 == 0, total_count.2 == 0) ]
#NOPE

#Thus, because these data are gear level, this is agg'd across species so relatively useless.

#drop these columns
mn_effort_18aug2023[ , `:=` ("total_count.1" = NULL,"total_count.2" = NULL) , ]





#fix up dates
mn_effort_18aug2023[ , date_clean :=  as.IDate(word(date.1, 1, sep = fixed(" ")), format = "%m/%d/%Y"),]
mn_effort_18aug2023[ is.na(date_clean), ]

mn_effort_18aug2023[ , hist(yday(date_clean)) ,]
mn_effort_18aug2023[ , hist(year(date_clean)) ,]

#check other fields
names(mn_effort_18aug2023)


#tidy sampling methods
mn_effort_18aug2023[ , .N , sampling_method ]
mn_effort_18aug2023[ , .N , .(sampling_method_abbrev)]

#keep the abbreviation/code column because that's the only thing in the CPUE file
mn_effort_18aug2023[ , .N , sampling_method_abbrev , ]
mn_cpue_21aug2023[ , .N , sampling_method_abbrev]

any(is.na(match(mn_effort_18aug2023[ , unique(sampling_method_abbrev),], mn_cpue_21aug2023[ , unique(sampling_method_abbrev) , ])))
#all abbreviations are covered (we could come back to that to add full gears to the cpues)

sort(names(mn_effort_18aug2023))

#year
mn_effort_18aug2023[ , unique(year) , ]
mn_effort_18aug2023[ is.na(year),]
 sum(mn_effort_18aug2023[ ,year != year(date_clean)])
#non-issue, delete year column
 
mn_effort_18aug2023[ , year := NULL] 
 

#effort units
mn_effort_18aug2023[ , .N , sampling_method ]


mn_effort_18aug2023[ , summary(total_effort_1.1) ,]

mn_effort_18aug2023[ total_effort_1.1 <0 , , ]

ggplot( mn_effort_18aug2023[!total_effort_1.1 < 0 ], aes(total_effort_1.1, group = sampling_method))+
  geom_density()+
  facet_wrap(~sampling_method, scales = "free")

#we can get units for these effort data from here (p45ish):
# https://files.dnr.state.mn.us/publications/fisheries/special_reports/180.pdf



mn_effort_18aug2023[ , unique(effort_units.1) ,]
effort[effort_units.1 == "knownunit_number_of_nets" , effort_units.1 := "knownunit_number_of_net_nights" , ]


#unique row key fields can be dropped
# these weights are associated with multiple fish, so maybe a mean or median--we're going to drop them (we can't assess anything from aggregated weights)
effort[ , `:=` (unique_row_key.1 = NULL,
               unique_row_key.2 = NULL,
               unique_row_key.3 = NULL,
               unique_row_key.4 = NULL)]



#rbindlist of the species matrix resulted in an imperfect fill of zeros in non EF data (should only be 0 or n_taxa(129) ):
unique(rowSums(is.na(effort[ , .SD , .SDcols = names(effort)[str_detect(names(effort), "taxa_")] ])))

#for all non EF data, fill NAs with zeros
setnafill(effort, fill = 0, cols = colnames(effort)[str_detect(names(effort), "taxa_")] ) #fill all of species matrix w/ zeros
#ef data now have zero catches in the taxa_cols, lets overwrite those w/ NAs
effort[new_file_name == "mn_ef_lmb_smb_effort_26Aug2022", names(effort)[str_detect(names(effort), "taxa_")] := NA , ]

#check work
unique(rowSums(is.na(effort[ , .SD , .SDcols = names(effort)[str_detect(names(effort), "taxa_")] ])))

```



# CPUE 2(3) ways
We have CPUE directly form the DNR, we can generate our own cpue using the individual fish data (both including and excluding the fish marked for CPUE inclusion)
## ISSUE?
```{r}

  #cpue data
  mn_cpue_21aug2023[  , .N , .(lake_id, date.1, sampling_method_abbrev, survey_type.1, species.1) ]
  sum(duplicated(mn_cpue_21aug2023))
  #not really sure why there would be duplicated cpues in there.. prob need to email corey gevingon this one. 
  mn_cpue_21aug2023[duplicated(mn_cpue_21aug2023) , .N, .(lake_name.1, lake_id, date.1) ]
  #if these cpues are calc'd per net or instance, that could cause it. how many of each gear in each survey?
  mn_cpue_21aug2023[ , .N, .(lake_name.1, lake_id, date.1, species.1, sampling_method_abbrev) ][, summary(N)]
  mn_cpue_21aug2023[ ,mean(total_effort_1.1), .(sampling_method_abbrev) ]
  mn_cpue_21aug2023[ , .N, .(lake_name.1, lake_id, date.1, species.1, sampling_method_abbrev) ][N>1, ]

  #not likely a problem. I think sometimes multiple rows are reported for a given gear on a survey, when the catch is == between two of them you get an aparrent "duplicate"
  
  #we'll be circling back to this dataset as a way to get the historical un-lengthed fish data into our fish-obs-as-rows format

```




Walleye GN example
```{r}
#how many walleye expected in our results from GN? This number matches that generated on 8 June 
sum(effort[ str_detect(sampling_method, "gill net")  , taxa_walleye , ])


#any surveys with nothing caught?
names(effort)[str_detect(names(effort), "taxa_")]
any(rowSums(effort[ , .SD , .SDcols = names(effort)[str_detect(names(effort), "taxa_")] ], na.rm =T)==0)
nrow(effort[new_file_name != "mn_ef_lmb_smb_effort_26Aug2022"][rowSums(effort[new_file_name != "mn_ef_lmb_smb_effort_26Aug2022", .SD , .SDcols = names(effort)[str_detect(names(effort), "taxa_")] ], na.rm = T)==0])

# add a nothing caught and a no_aggd_catch_data column
effort[new_file_name != "mn_ef_lmb_smb_effort_26Aug2022" , nothing_caught := rowSums(effort[new_file_name != "mn_ef_lmb_smb_effort_26Aug2022" , .SD , .SDcols = names(effort)[str_detect(names(effort), "taxa_")] ])==0  ,]
effort[new_file_name == "mn_ef_lmb_smb_effort_26Aug2022", no_aggd_catch_provided := TRUE , ]
effort[new_file_name != "mn_ef_lmb_smb_effort_26Aug2022", no_aggd_catch_provided := FALSE]


effort[nothing_caught==T] #2 surveys really did catch NADA
# Holly - What did these look like in the raw data?

#now expand to long data:(this code generates HUGE files) We should skip this step. Instead, lets set aside that species catch matrix, and use it later in the cross

# effort_expanded <- melt(effort, measure.vars = patterns("^taxa"), variable.name = "species", value.name = "count"  )[order(lake_id, date_clean, sampling_method, survey_type.1), ,]
# effort_expanded[species == "taxa_walleye", sum(count, na.rm = T)] #529259
# effort_expanded_presences_only <- uncount(effort_expanded[no_aggd_catch_provided==F], count, .remove = F, .id = "id") # this will drop all zeros, must include a subset excluding the data for which counts were not provided--this will otherwise snag the weights arg (can't weight by NA)
# effort_expanded[count==0, id := NA]
# nrow(rbind(catch_expanded[count==0],catch_expanded_presences_only))
# catch_expanded <- rbind(catch_expanded[count==0],catch_expanded_presences_only)
# rm(catch_expanded_presences_only)

keycols_and_catch_mat <- c("lake_id", "date_clean", "sampling_method", "survey_type.1", names(effort)[str_detect(names(effort), "taxa_")])
DNR_calcd_catch <- effort[  , .SD , .SDcols = keycols_and_catch_mat  ]
rm(keycols_and_catch_mat)

#drop catch mat from effort data
effort[ , c( names(effort)[str_detect(names(effort), "taxa_")]) := NULL , ]

#clean some crumbs out of ws:
rm(mn_ef_lmb_smb_effort_26Aug2022, mn_fish_effort_03May2022, mn_gde_gsh_fish_effort_03May2022)

#clean up lake_ids
effort[ , lake_id_chr := lake_id ]
effort[ , lake_id := as.integer(lake_id)]


```


# Merge Indiv Fish Data

```{r}

#now tie the indiv fish data to these effort records

# the EF fish data have corrupted fish attributes (so we'll overwrite all of those with NAs for now)
badcols <- c("sample_id", "length.1", "weight.1", "age", "young_of_year", "sex", "reproductive_condition", "gear_data_notes.1" )
mn_ef_lmb_smb_catch_26Aug2022[ , (badcols) :=  NA , ]

#check work
mn_ef_lmb_smb_catch_26Aug2022

# make EF survey_id into the same data format as others
mn_ef_lmb_smb_catch_26Aug2022[ , survey_id := as.integer64(survey_id) ,]

#first, merge the indiv fish data
indiv_fish <- rbindlist(list(mn_ef_lmb_smb_catch_26Aug2022, 
                             mn_r1_gillnet_indiv_fish_20oct2020,
                             mn_r2_gillnet_indiv_fish_20oct2020,
                             mn_r3_gillnet_indiv_fish_20oct2020,
                             mn_r4_gillnet_indiv_fish_20oct2020,
                             mn_shallgillnet_indiv_fish_20oct2020,
                             mn_deepgillnet_indiv_fish_20oct2020,
                             mn_r1_trapnet_indiv_fish_20oct2020,
                             mn_r2_trapnet_indiv_fish_20oct2020,
                             mn_r3_trapnet_indiv_fish_20oct2020,
                             mn_r4_trapnet_indiv_fish_20oct2020),
                               fill = TRUE,
                               use.names = TRUE) #will have to check this tomorrow, but seems like it worked

#inspect merge
colnames(indiv_fish)
sort(colnames(indiv_fish))

setcolorder(indiv_fish, c("lake_id", "date.2", "date.1", "sampling_method_abbrev", "survey_type.1")) 

indiv_fish[ , .N , .(sampling_method_abbrev, species.1)  ]
indiv_fish[ species.1 == "WAE", .N , .(sampling_method_abbrev)  ] # here we can see that we've not got "all gears" data bc WAE have def been captured in EF (and other gears)
indiv_fish[ species.1 == "WAE", .N , .(sampling_method_abbrev)  ][ , sum(N) , ]

#drop data inputs for fish data
rm(mn_ef_lmb_smb_catch_26Aug2022, 
                             mn_r1_gillnet_indiv_fish_20oct2020,
                             mn_r2_gillnet_indiv_fish_20oct2020,
                             mn_r3_gillnet_indiv_fish_20oct2020,
                             mn_r4_gillnet_indiv_fish_20oct2020,
                             mn_shallgillnet_indiv_fish_20oct2020,
                             mn_deepgillnet_indiv_fish_20oct2020,
                             mn_r1_trapnet_indiv_fish_20oct2020,
                             mn_r2_trapnet_indiv_fish_20oct2020,
                             mn_r3_trapnet_indiv_fish_20oct2020,
                             mn_r4_trapnet_indiv_fish_20oct2020)



#to-do:
# make these clean: "lake_id", "date_clean", "sampling_method", "survey_type.1"

#dates (from data explainer we know that date.2 is survey date, but only available in non-EF files)
indiv_fish[ , .(date.2, date.1), ]
indiv_fish[ , .N , .(date1=is.na(date.1), date2=is.na(date.2)) ]

#change dates to Idate:
indiv_fish[ , date_clean := as.IDate(date.2, format = "%m/%d/%Y") ,]

indiv_fish[ , summary(date_clean) , ]
indiv_fish[is.na(date_clean) == T , .N ]

indiv_fish[is.na(date_clean) == T , date_clean := as.IDate(date.1, format = "%m/%d/%Y") ,  ]

#check our work:
indiv_fish[ , summary(date_clean) , ]


#lake_id
indiv_fish[ , summary(lake_id) ,]
indiv_fish[ , str(lake_id), ]
effort[ , str(lake_id) ,]
#we can anticipate this being a problem when we go to merge the data


#tidy sampling methods
indiv_fish[ , .N , .(sampling_method_abbrev)]
effort[ , .N , sampling_method]
# git issue on this: inidv fish dat include backpack EF, but effort data do not

#create the sampling_method column from codes:
indiv_fish[sampling_method_abbrev == "GSH",
      sampling_method := "Shallow gill nets"]
indiv_fish[sampling_method_abbrev == "GDE",
      sampling_method := "Deep gill nets"]
indiv_fish[sampling_method_abbrev == "TN",
      sampling_method := "Standard trap nets"]
indiv_fish[sampling_method_abbrev == "GN",
      sampling_method := "Standard gill nets"]
indiv_fish[sampling_method_abbrev == "EF",
      sampling_method := "Standard electrofishing"]
indiv_fish[sampling_method_abbrev == "SEF",
      sampling_method := "Special sampling, electrofishing"]
indiv_fish[sampling_method_abbrev == "EFB",
      sampling_method := "Backpack electrofishing"]

#check
indiv_fish[ , .N , .(sampling_method_abbrev, sampling_method)]

#ditch the abbreviation/code column
indiv_fish[ , sampling_method_abbrev := NULL, ]

#survey.type

indiv_fish[ , .N , survey_type.1 ]
effort[ , .N , survey_type.1]

indiv_fish[ , .N , survey_type.1 ][ ,survey_type.1 , ]


# generate a type table (these codes can be found in MN_Data folder)
survey_type <- data.table(code = indiv_fish[ , .N , survey_type.1 ][ ,survey_type.1],
                          fullname = effort[ , .N , survey_type.1][ ,survey_type.1 ][c(2,4,3,9,1,10,7,5,6,12,8,14,11,13) ]
                                                         )
#execute
match(indiv_fish[ , survey_type.1], survey_type[,code])
survey_type[match(indiv_fish[ , survey_type.1], survey_type[,code]), fullname]

indiv_fish[ , survey_type.1 := survey_type[match(indiv_fish[ , survey_type.1], survey_type[,code]), fullname]   ,]
rm(survey_type)


# add an id to the indiv fish
indiv_fish[ , id := seq_len(.N) , .(lake_id, date_clean, sampling_method, survey_type.1, species.1) ]
indiv_fish[ , summary(id) ,]


#summarize ages
indiv_fish[ , .N , age ]
indiv_fish[ , .N , .(age=!is.na(age))]
#compare to the aged fish file:
#how many aged fish in the age data from Years we havem Gears we have, and lakes we have?
mn_aged_fish_v2_20apr2023[year(as.IDate(date.1, format = "%m/%d/%Y"))<2020 &
                            sampling_method_abbrev %in% c("GN", "GSH", "GDE", "TN", "EF", "SEF", "EFB") &
                            lake_id %in% indiv_fish[ ,unique(lake_id)], .N ,]
#same, now by year:
mn_aged_fish_v2_20apr2023[year(as.IDate(date.1, format = "%m/%d/%Y"))<2020 &
                            sampling_method_abbrev %in% c("GN", "GSH", "GDE", "TN", "EF", "SEF", "EFB") &
                            lake_id %in% indiv_fish[ ,unique(lake_id)], .N , year(as.IDate(date.1, format = "%m/%d/%Y"))]

#make an issue for this
#we need to check this out further, BUT generally 7/8 of the aged fish are represented in the indiv fish data we recieved

#merge the effort and indiv fish data


#scope the merge:
#99% of the wb in the effort data are represneted in indiv fish data
sum(indiv_fish[ , unique(lake_id) , ] %in% effort[ , unique(lake_id)])/effort[, length(unique(lake_id)) ,] 

#99% of the wb in the indiv fish data are represneted in  effort data
sum(indiv_fish[ , unique(lake_id) , ] %in% effort[ , unique(lake_id)])/indiv_fish[, length(unique(lake_id)) ,] 

# come back and scope out each individual key column match
# #20% coverage on the survey id column--this checks. Corey Geving said to stay away from these
# sum(age[ , .N , survey_id][,survey_id] %in% catch_expanded[ , unique(survey_id)])/age[, length(unique(survey_id)) ,] 
# 
# #49% coverage on the survey id column--this checks. Corey Geving said to stay away from these
# sum(age[ , .N , date_clean][,date_clean] %in% catch_expanded[ , unique(date_clean)])/age[, length(unique(date_clean)) ,] 

#we know that dates are a problem, can we schmooze that join?
# flatfish <- merge(catch_expanded,age, by = c("lake_id", "date_clean" , "sampling_method", "id", "species"), suffixes = c("_catch", "_age"))

flatfish <- merge(effort,indiv_fish, by = c("lake_id", "date_clean" , "sampling_method", "survey_type.1"), suffixes = c("_effort", "_indivfish"), all = T)

flatfish[ , .N , sampling_method]

flatfish[ (species.1 == "WAE" & sampling_method %in% c("Standard gill nets", "Shallow gill nets", "Deep gill nets")), .N, .(year = year(date_clean), age = !is.na(age), length = !is.na(length.1))][order(year, -N)]


#this chunk is what Mike thinks GH wants for today
names(flatfish)

#how many unique surveys exist for each category of has effort and has indiv fish data?
flatfish[ , length(unique(paste(lake_id, date_clean , sampling_method, survey_type.1))) , .(effort = !is.na(state_effort), indiv_fish_dat = !is.na(state_indivfish)) ]
  
#sum of all
  flatfish[ , length(unique(paste(lake_id, date_clean , sampling_method, survey_type.1))) , .(effort = !is.na(state_effort), indiv_fish_dat = !is.na(state_indivfish)) ][ , sum(V1) , ]

#for each of those with both effort and fish data, summarize catch by species (bring effort along too!)   
flatfish[ !is.na(state_effort)&!is.na(state_indivfish) , .N , .(lake_id, date_clean , sampling_method, survey_type.1, species.1, total_effort_1.1, effort_units.1) ]

  ## SIDEBAR! Flagging suspected issues
  #summarize the total effort for ef data
  flatfish[ effort_units.1 == "knownunit_hours" , hist(total_effort_1.1, breaks = 100) , ]# there are some very wierd values in here! Def want to flag these for review by a user! Here's an example of that:
  flatfish[ effort_units.1 == "knownunit_hours" , summary(total_effort_1.1) , ]
  flatfish[ effort_units.1 == "knownunit_hours" & !is.na(total_effort_1.1) , .(mean(total_effort_1.1) , sd(total_effort_1.1)) , ] #find sd and mean
  flatfish[ effort_units.1 == "knownunit_hours" & total_effort_1.1 > 1.464117 + (5*flatfish[ effort_units.1 == "knownunit_hours" & !is.na(total_effort_1.1) , sd(total_effort_1.1) , ])   , flagged_effort := TRUE , ] #flag records > 5 sd from mean
  flatfish[ , .N , flagged_effort]
  
  
#cast summarized catch data wide:
wide_complete <- dcast(flatfish[ !is.na(state_effort)&!is.na(state_indivfish) , .N , .(lake_id, date_clean , sampling_method, survey_type.1, species.1, total_effort_1.1, effort_units.1) ], ... ~ species.1 , value.var = "N", fill = 0)

# calc cpue for all wae
wide_complete[ , .(lake_id, date_clean , sampling_method, survey_type.1, total_effort_1.1, effort_units.1, WAE) ,]
wide_complete[ , wae_cpue := WAE/total_effort_1.1 , ]
# check mean cpue for gillnets
wide_complete[ str_detect(sampling_method, "gill net") , mean(wae_cpue) , sampling_method ]

  
flatfish[ species.1 == "WAE" , .N ,]

#where lake Id is a kittle no, we can drop those data (lake_id blank, lake_id_chr has kittle)
flatfish[ , .N , .(lake_id, lake_id_chr, date_clean , sampling_method, survey_type.1, total_effort_1.1, species.1)] 
  flatfish[!is.na(lake_id), .N , .(lake_id, lake_id_chr, date_clean , sampling_method, survey_type.1, total_effort_1.1, species.1)]
  flatfish <- flatfish[!is.na(lake_id)]

#lets get a cpue for walleye
  flatfish[ , , .(lake_id, date_clean , sampling_method, survey_type.1) ][]
    
  
flatfish[ , summary(date_clean) ,]

flatfish[ , hist(year(date_clean)) , ]

#walleye data product:
wae_complete_surveys <- wide_complete[ , .(lake_id, date_clean , sampling_method, survey_type.1, total_effort_1.1, effort_units.1, WAE) ,]
  
wae_records <- flatfish[ species.1 == "WAE" , ,] 

wae <- merge(wae_complete_surveys, wae_records, all = T, by = c("lake_id", "date_clean" , "sampling_method", "survey_type.1", "total_effort_1.1", "effort_units.1"))

wae[ , .N , .(lake_id, date_clean , sampling_method, survey_type.1, total_effort_1.1, effort_units.1, WAE) ] 


wae_gn_records <- wae[ str_detect(sampling_method, "gill nets") , , ]





#save to disk:

# saveRDS(effort, file = "Data_and_Scripts\\Data\\output\\mn_effort.rds")
# saveRDS(DNR_calcd_catch, file = "Data_and_Scripts\\Data\\output\\mn_dnr_calcd_catch.rds")
# saveRDS(indiv_fish, file = "Data_and_Scripts\\Data\\output\\mn_indivfish.rds")
# saveRDS(flatfish, file = "Data_and_Scripts\\Data\\output\\mn_flat_effort_indivfish_merge.rds")
# saveRDS(wae_gn_records, file = "Data_and_Scripts\\Data\\output\\mn_wae_gn.rds")




```




```{r}
# review what's available for WAE data, esp to compare to kelsey V's work:

wae_gn_records[ year(date_clean)%in% c(2018:2022)  , .N , .(year(date_clean), cpue = !is.na(count) ,length = !is.na(length.1), age = !is.na(age)) ][order(year)]



mn_fish_2022only_31May2023[ , .N , .(length = !is.na(length.1), age = !is.na(age) )  ]

KV_WAE <- fread("C:\\Users\\verh0064\\Desktop\\2019_WAE_RAW_ALLREGIONS_20200505.csv")

colnames(KV_WAE)
KV_WAE[ , .N , as.integer(FISH_COUNT)]

KV_WAE[ , FISH_COUNT := as.integer(FISH_COUNT) ,]

KV_WAE <-  uncount(KV_WAE, FISH_COUNT, .id = "id", .remove = T  )

KV_WAE[ , date_clean := as.IDate(SRVY_DT, format = "%m/%d/%Y") ,]

KV_WAE[ , .N , .(catch = !is.na(CPUE), length = !is.na(LEN_MM), weight = !is.na(WT_G), age = !is.na(OFF_AGE))]
KV_WAE[ , .N , .(year = year(date_clean) , catch = !is.na(CPUE), length = !is.na(LEN_MM), age = !is.na(OFF_AGE))]



# lets explore some unmatched lengths and see if there are catch data that correspond:

wae_gn_records[is.na()]


raw_agedfish <- fread("C:\\Users\\verh0064\\Desktop\\all_aged_fish_GH_2023_nullS.txt")

raw_agedfish[ , .N , OFFICIAL_AGE ][order(OFFICIAL_AGE)]
raw_agedfish[ , .N , is.na(OFFICIAL_AGE) ]





```

```{r}

library(vegan)

colnames(wide_complete)
wide_complete[ , 7:112 ,]

diversity(wide_complete[ , 7:112 ,], index = "invsimpson" )


wide_complete[ , invsimpson := diversity(wide_complete[ , 7:112 ,], index = "invsimpson" ) ,]

wide_complete[ , alltime_masIS:= round(max(invsimpson),0)  ,  .(lake_id, sampling_method)]
wide_complete[ , alltime_medIS:= median(invsimpson)  ,  .(lake_id, sampling_method)]



ggplot(wide_complete[sampling_method %in% c("Standard gill nets", "Standard trap nets")] , aes(year(date_clean), invsimpson)  )+
  geom_line(aes(group = lake_id))+
  facet_grid(alltime_masIS~sampling_method)+
  geom_smooth()


ggplot(data = wide_complete[ , .(var(invsimpson) , alltime_medIS), lake_id ], aes(alltime_medIS, V1))+
  geom_point()+geom_smooth()+ ylab("variance in Diversity") + xlab("alltime median Diversity")

ggplot(data = wide_complete[ , .(var(WAE/total_effort_1.1) , alltime_medIS), lake_id ], aes(alltime_medIS, V1))+
  geom_point()+geom_smooth()+ ylab("variance in walleye cpue") + xlab("alltime median Diversity")+scale_y_log10()
















```
