---
title: "IA_Flat_File_Aggregation"
author: "Mike Verhoeven"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---
# Preamble

## To Do List
    1. Need to get a proper effort file. Current fish obs doesn't seem to have many zeros
    


##Instructions
This code is written in chunks that each accomplish a task moving towards the goal of a file for each state that encompasses all fish observations made/shared with us. The structure of that observation-level data should be one row per individual fish. By joining these data to the effort info, we will be able to filter and aggregate the data in flexible ways, always bringing along info on how much effort it took to catch each the fish obs (or set of fish).

The data explainer sticks .n suffixes on columns where multiple fields from one of the datastes has multiple cols that match that field (i.e., date.1, date.2, date.3). The naming conventions for cols uses "_" and no spaces.

After loading packages, the data from each state will be loaded into the WS and renamed according to the mapping of old colnames to new colnames in the data explainer. Next, the files should be explored a bit, and the script should identify files that will not be used, but instead get removed from the workspace. After this initial exploration fo what's there, the files should be restructured and munged into the obs-level format described above. When this is done, subsequent blocks should conduct some baseline additional QC should be done to verify the product of the munging is as-expected. Finally, the script should tidy up an remaining column or field formatting (e.g., species uses common names, no spaces, but "_"), and drop an unneeded columns. 


A basic guide to columns we expect to see in a observation level data are as follows:

LOCATION INFORMATION:
state - 
county - county associate with the wb in the state data
lake_name - common lang name of the lake
lake_id - usually a local id specific to the state contributing the data
nhdhr.id - This column is usually added towards the end of the script based on state lake_ids using the mwlaxeref (Paul Frater) package from here: https://drive.google.com/drive/u/1/folders/1HURmPTtufVzI0aqn7D8MpKdL5B8atCL5

SURVEY INFORMATION:
date_clean - usually multiple dates are submitted with each fish (e.g., collection date, survey end date). Use the date of the survey as the primary date for each fish observation, generating a date_clean column
survey_type - this is often specified in the data, and sometimes helps to filter out which data are useful for any given purpose (e.g., research survey, fishkill check)
survey_id - in some states this is a provided variable used as a key to each "survey." Ususally a "survey" is multiple gears on a single lake on a single date (often surveys might run multiple consecutive dates, but only one date is reported )
sampling_method - This is a gear field, and often includes wide ranging gears and sometimes very specific gears
total_effort.1 - This should be a numeric field with only the qty of effort
effort_units.1 - paired with total_effort.1, defines units for numeric
nothing_caught - specifies that nothing was caught in this effort (species will also be NA)
target_species - what was the species being targeted in the survey?
effort_ident - This is a field we add, it is a unique key for each effort unit that we have data for(usually a gear within a survey). For example, a data user could get cpue by counting all fish within a group_by(effort_id) or it's equivalent group_by(lake_id, date, survey_type,sampling_method) 

TAXA INFORMATION: 
species.1 - species common name
species_abbrev - State level code sometimes used in data share
length.1 - length of fish observed, numeric
length_unit.1 - units for length.1, also specify resolution if needed (e.g, cm, whole cm)
weight.1 - weight of fish obs, numeric
weight_unit.1 - units of weight.1, also specify resolution if needed (e.g, lb, whole lb)
sample_id.1 - unique id for each fish observation sometimes provided and sometimes useful for connecting to aged fish
age - age in years, numeric
aging_structure - what was used to determine age?
young_of_year - was the fish a YOY (i.e. hatched <365d before surveyed)
sex - sex of fish (male, female, unknown, NA)

SOURCE FILE INFORMATION: These columns come in with each dataset from the data explainer and we leave them in the product so that we could hunt down issues we find a bit more easily. 

original_file_name.1_effort - name of effort file that was used to generate data in this row
original_file_name.1_indivfish - name of individual fish file that was used to generate data in this row
original_file_name.1_[...]

FLAGS AND ISSUES:
flag - this column contains a character string with issues describing each row, each issue separated with a comma. Use mutate(flag = paste(flag, "new issue description", sep = ",")) to add to this column without overwriting other issues already specified.







##Libraries
```{r}
library(arrow)
library(readr)
library(dplyr)
library(stringr)
library(data.table)
library(janitor)
library(tidyr)
library(lubridate)
library(bit64)

options(scipen = 999)
```


##Data
This could readily be changed into a function that takes a filepath and returns files into environment.
* note Holly has to change file paths to "D" 
```{r}
#generate a file list to import
files_list <- list.files(path = "E:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/IA_Data/ia_raw_disaggregated_data", pattern = ".+\\.csv") #grabs only.csv files
files_list



#object for use in loop (simple length of file list)
n <- length(files_list)

for(i in 1:n) {
  #i = 3
  filei <- word(gsub(".csv","", files_list[i]), start = -1, sep = fixed("/"))
  #this does those two steps in one package
  assign(filei ,
          fread(paste0("E:/Shared drives/Hansen Lab/RESEARCH PROJECTS/Fish Survey Data/IA_Data/ia_raw_disaggregated_data/",
                                          files_list[i])))
  
  # if the file is a crosswalk, do not rename anything, just loop to the confirm import line
  if(str_detect(filei, "crosswalk")) {  #confirm import of files:  
    print(paste(filei ,"added to workspace" ))  
    #confirm import of files:  
    print(paste(i ,"files added to workspace" )) ; next}
  
  #if the file is not in the data explainer, don't try to rename it:
  if(filei %in% cde$new_file_name) {
    print("renaming with data explainer")
  } else {next}
  
  
  
  
  
  # note we want to review a sorted list of column names to check misspelling etc.
  # we still need to use the columns with names like col_name_length_in, or known_units
  
  
  cde %>% # call data explainer file
    filter(`new_file_name`== filei)%>% #keep only the row relevant to this file
    select_if(~ !any(is.na(.))) %>% 
    data.table::transpose(keep.names = "newname") %>% 
    rename("oldname" = V1) %>% 
    assign("names", ., envir = .GlobalEnv)
  
  #see if any column names will not have a match! 
  # IF any pop FALSE, force stop and revist of data explainer ()
  # - e.g., named something "total catch" when actual column name was "total_catch"
  print(
    cbind(colnames(get(filei)),
          colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]
    )
  )
  
  
  # break the loop if the current file has column names not in the data explainer
  # if (all(cbind(colnames(get(filei)),  colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ])[,2]) == FALSE ) break
  if (all(colnames(get(filei)) %in% names[ !str_detect(newname,"unique_row_key"), oldname, ]) == FALSE ) break
  
  
  # append old col names into new "notes" columns:
  get(filei)[ , (names[ str_detect(newname, "notes") , oldname   ,  ]) := Map(paste, colnames(.SD), .SD, sep = ':') , .SDcols =  names[ str_detect(newname, "notes") , oldname   ,  ] ]
  
  #now rename that file's colnames
  setnames(get(filei), colnames(get(filei)), names[!str_detect(newname,"unique_row_key")] [match(names(get(filei)),names[!str_detect(newname,"unique_row_key"),oldname]), newname] )
  
  #append all other data from data explainer
  unusedbits <- 
    data.table(
      matrix(
        rep(names[ !newname %in% colnames(get(filei)) , oldname , ],
            each = nrow(get(filei))
        ),
        nrow = nrow(get(filei)),
        dimnames = list(rep(NA,nrow(get(filei))),
                        names[ !newname %in% colnames(get(filei)) , newname , ])
        )
      )
  
  #add all not yet used columns from data explainer:
  get(filei)[ , (names[ !newname %in% colnames(get(filei)) , newname , ]) := unusedbits[] ]

  #confirm import of files:  
  print(paste(filei ,"added to workspace" ))  
  #confirm import of files:  
  print(paste(i ,"files added to workspace" )) 

  
} 
  #confirm import of files:  
  print(paste(i ,"files added to workspace" ))
  #confirm import of files:  
  print(paste(n-i ,"remaining to be added" )) 



```


#Data Review
```{r}

# review each dataset that we have, strategizing about how you'll use them to develop a obs-level file
# consider things like species scope (do I need to restrict all input data to just the 8ish game species?), file organization (is this file already in a obs-level format or is it a count of each species/size that I should uncount()?), linking keys (is there a fish obs ID in the age data that I can use to link to the fish observations data?), and what things (posisbly whole datasets) are unneeded for our work, here. I have left the Michigan work in here to give you an idea of what I did:


#fish obs
glimpse(ia_fishlengths_24Oct2023)

#do these data contain any no_fish_caught surveys?
ia_fishlengths_24Oct2023[total_count.1 == 0, .N] #there are a few zeroes. Let's investigate what they mean...

ia_fishlengths_24Oct2023[ , .N , survey_id][ , summary(N)] #every survey has at least one line
ia_fishlengths_24Oct2023[ , .("countfish" = sum(total_count.1)), survey_id][ , summary(countfish) ,] #none of the surveys show no fish caught
#any_no_fish_caught in a gear within a survey situation?
ia_fishlengths_24Oct2023[, .("countfish" = sum(total_count.1), "countspp" = length(unique(species.1))), site_id.1][ , summary(countfish)] #none of the site_ids seem to show zero either

ia_fishlengths_24Oct2023[, .(.N, "countfish" = sum(total_count.1)), .(site_id.1, species.1)][ ,summary(countfish) ] # again, counting N is not relevant bc we're used binned counts. BUT ..there are some zeros...

ia_fishlengths_24Oct2023[, .(.N, "countfish" = sum(total_count.1)), .(site_id.1, species.1)][countfish == 0 , ,]
ia_fishlengths_24Oct2023[ total_count.1 == 0 , .N,  ]
# The zeros that are in here are very limited and I'd suggest they are wonky and inconcistent. 

# No these data do not include seemingly proper zeros. We'll reach out to the IA folks to get a new dataset that gives us all the CPUEs


# #but for now, I dug Colin Dassow's old cpue file up. Whats that contain? NOte here that the data rip I'm working with in IA has some Active v. non-active status issue that means I can't see all of the fish data. 
# ia_cpue_21Aug2021[cpue == 0, .N , ] #almost certainly these are wonky zeros too (incomplete)
# #here's "all" the survey work:
# ia_cpue_21Aug2021[ , .N , .( date.1, lake_id,  sampling_method) ]
#and here's whats in the fish lengths:
ia_fishlengths_24Oct2023[ , date_clean := as.IDate(date.1, format = "%m/%d/%Y") , ]
# ia_cpue_21Aug2021[ ,date_clean := as.IDate(date.1, format = "%m/%d/%Y")]

#There's no point in comparing these things because we know that the current IA fish obs file does not contain any of the surveys woth status == NOT ACTIVE in the online database/portal 
# ia_cpue_21Aug2021[ , .N , .( date.1, lake_id,  sampling_method) ][!(ia_fishlengths_24Oct2023[ , .N , .(date.1, lake_id,  sampling_method)]), on = .( date.1, lake_id,  sampling_method) , ]


##Back to the fish data:
ia_fishlengths_24Oct2023[ , .N , .(date.1, date_clean, lake_id,  sampling_method)] #check the date cleaning
summary(ia_fishlengths_24Oct2023$date_clean) #whoa... the years actually seem reasonable...

#where there are surveys showing zeros, are they there as place holders?
ia_fishlengths_24Oct2023[total_count.1 == 0, .(survey_id, duplicated(survey_id)) , ]
ia_fishlengths_24Oct2023[survey_id%in%ia_fishlengths_24Oct2023[total_count.1 == 0, c(survey_id) ], .N , survey_id 
]
#negative. those are not zero surveys. how about for the site_id.1
ia_fishlengths_24Oct2023[site_id.1%in%ia_fishlengths_24Oct2023[total_count.1 == 0, c(site_id.1) ], .N , site_id.1 
]
#not relevant to this cause either!
#generally, this shows us that the zeros that are in here are somewhat meaningless to our cause.




```

## Restructure the fish obs
```{r}
#reorganize this beast:
cols <- as.character(expression(
    survey_id, state, county, lake_id, lake_name.1, date_clean, date.1,
    site_id.1, sampling_method, total_effort_1, distance, 
    sample_id.1, species.1, length.1, length_unit.1, weight.1, weight_unit.1, sex, total_count.1,
    aging_structure.1, aging_structure.2, aging_structure.3, 
    start_time,
    lake_type, location_notes.1, lat_unspec, lon_unspec, 
    original_file_name.1))

setcolorder(ia_fishlengths_24Oct2023, cols)


# so if we want a fish as rows setup, we uncount on total_count.1 column. How many rows will that generate? Nearly 1m. 
ia_fishlengths_24Oct2023[ , sum(total_count.1) , ]

#note that we don't care if we drop the zeros that were in here. We don't think they are useful/meaningful anyway.
ia_fishlengths <- uncount(ia_fishlengths_24Oct2023, weights = total_count.1, .remove = T, .id = "uncount_id")

# of fish per survey
ia_fishlengths[ , .N , survey_id]
# of species per survey
ia_fishlengths[ , length(unique(species.1)) , survey_id ]
# check that survey Id does what I think it does
ia_fishlengths[ , length(unique(lake_id,lake_name.1,date_clean)), survey_id ][V1 > 1]# Because there are no cases where a survey id contains multiple cases of lakeXdate, thi suggests to me survey ID does indeed fit with what we think it does

#check other basic cols
ia_fishlengths[ ,.N , state ]
ia_fishlengths[ , .N , county ]
ia_fishlengths[ , .N , .(lake_id, lake_name.1)]
  ia_fishlengths[ , unique(lake_name.1) , ] #do all lakes have that goofy code in the name?
  ia_fishlengths[ , lake_name.1 := word(lake_name.1, 1, -2),] #keep words in the string starting at word 1, ending at second to last
ia_fishlengths[ , .N , year(date_clean) ]
  
# site_id is like total_effort_ident, right?
ia_fishlengths[ , length(unique(sampling_method, survey_id)) , site_id.1][V1>1] #sure looks like it based on this!!!
ia_fishlengths[ , .N , .(site_id.1, sampling_method, survey_id, total_effort_1) ] # and we seem to have ONE effort value for each of these! What a miracle!

#sampling method
ia_fishlengths[ , .N , sampling_method ]


#fish lengths unpacked, clean up ws
rm(ia_fishlengths_24Oct2023, ia_diet_21Aug2021, ia_lake_characteristics_21Aug2021, names, unusedbits, cde)




```


# Prep Age Data
```{r}
#We've got three age files, each of them is uniquely messy.

glimpse(ia_age_length_21Aug2021)
toString(names(ia_age_length_21Aug2021))
ia_age_length_21Aug2021[ , .N, is.na(date.1) ]
ia_age_length_21Aug2021[ , .N , survey_type.1]
ia_age_length_21Aug2021[ , .N , aging_data_notes.3]
ia_age_length_21Aug2021[ , .N , .(garbage_bin_notes.4, species.1)]


#drop a bunch of backcalc cols and other crud
cols <- as.character(expression(sample_id.1, sample_id.2, sex, lake_id, lake_name.1, county, date.1, garbage_bin_notes.1, garbage_bin_notes.2, garbage_bin_notes.3, year, sampling_method, species.1, length.1,length_unit.1, weight.1, weight_unit.1, age, aging_structure.1, garbage_bin_notes.4, original_file_name.1))
   
ia_age_length_21Aug2021 <- ia_age_length_21Aug2021[ , .SD , .SDcols = cols]
#there, now each fish is a row. Bring in other data


#these guys have multiple rows for each fish. We want only the age observed, not the backcalc ages
head(ia_BLG_age_length_21Aug2021)
glimpse(ia_BLG_age_length_21Aug2021)

ia_BLG_age_length_21Aug2021[ , .N , sample_id.1 ]

#how to select that: we don't need to do much. each row has the basic info, just take the first of each:

ia_BLG_age_length_21Aug2021[ , .SD[1] , sample_id.1 ]#in each subset print row 1
ia_BLG_age_length_21Aug2021 <- ia_BLG_age_length_21Aug2021[ , .SD[1] , sample_id.1 ] # in each subset print row 1

#nab out only useful cols:
toString(names(ia_BLG_age_length_21Aug2021))
cols <- as.character(expression(sample_id.1, garbage_bin_notes.1, lake_name.1, aging_structure.1, species.1, length.1, weight.1, sex, age, original_file_name.1, length_unit.1, weight_unit.1))
ia_BLG_age_length_21Aug2021 <- ia_BLG_age_length_21Aug2021[ , .SD , .SDcols = cols]







#date data
ia[, unique(year)]

#backfill year?
ia[is.na(year) & !is.na(date.1)]
#date align with year?
ia[!is.na(date.1) & !year==year(date.1)]

#lost in garbage can? yes there are some month and season data there, 
ia[is.na(date.1), date.2 := word(garbage_bin_notes.1, -1, sep = ":"), ]
ia[is.na(date.1), .N , date.2]
ia[date.2 == "7", date.2 := "July"]
ia[date.2 == "6", date.2 := "June"]
ia[date.2 == "8", date.2 := "August"]
ia[date.2 == "5", date.2 := "May"]
ia[date.2 == "NA", date.2 := NA  ]
ia[is.na(date.1)& is.na(date.2), date.2 := word(garbage_bin_notes.3, -1, sep = ":"), ]
ia[date.2 == "." , date.2 := NA, ]
ia[ ,date.2 := tolower(date.2) , ]

ia[is.na(date.1), .N , date.2]

ia[ , date.1 :=  as.IDate(date.1) , ]

ia[, date_clean :=  as.IDate(date.1)]

hist(ia[!is.na(date_clean) ,yday(date_clean)])

#now populate some dates where only mo or season provided (set to 15th if mo given, or season date approximated from histogram here^)

ia_datefill <- transpose(keep.names = "oldname", data.table(fall = "2 Oct",
                                                          july = "15 July",
                                                          spring = "20 April",
                                                          summer = "19 July",
                                                          june = "15 June",
                                                          august = "15 August",
                                                          may = "15 May"))
ia[ , date.3 := 
      ia_datefill[match(ia[ ,date.2],ia_datefill[,oldname]) ,
                V1 , ]
    , ]

ia[!is.na(year) , date.4 := paste(date.3, year)]

ia[ , .N, date.4]

ia[str_detect(date.4, "NA"), date.4 := NA]

ia[is.na(date_clean) & !is.na(date.4), date_clean := as.IDate(date.4, format = "%d %B %Y") ]

# check coverage
hist(ia[ ,yday(date_clean)])
ia[!is.na(date_clean), .N , ]/ia[ , .N , ]

rm(ia_datefill)

# make other dates character strings
datecols <- colnames(ia)[str_detect(colnames(ia), "date\\.")]
ia[    , (datecols) := lapply(.SD, as.character)    ,   .SDcols = datecols]

#in this chunk you join the effort data to the fish-as-rows or obs-level dataset. I start by prepping these multiple files for a merge. After each operation, be sure to check your work! Again I have left MI in here to give you an idea of how one previous example went. 

#grabbing effort from catch file


``` 
          
          
          
# Data tidying                        
```{r}                       
### dataset cleanup and tidying. MI work left here as an idea of a previous state's work



                                               
                        #check the product:
                        colnames(mi_catch_eff_merge)
                        
                       mi_catch_eff_merge[ str_detect(lake_name.1, "ike" ), .(count = .N, missinglengths = sum(is.na(length.1)), meanL = mean(length.1)), .(lake_name.1, lake_id, date.1_effort, date.1_mergedcatch, survey_id, sampling_method_abbrev, species.1) ]

            
                        #did we retain all of the surveys? Looks like 498 unique survey IDs, and both the product and input reflect this:
                        mi_catch_eff_merge[ , .N , .(survey_id)]
                        merge(merge(mi_statustrends_effort_16Mar2021[ , .( effort_nrows = .N) , survey_id], 
                                    mi_statustrends_catch_16Mar2021[ , .( catch_nrows = .N) , survey_id], all = T),
                              mi_statustrends_catchlengthclass_03July2023[ , .( catchlen_nrows = .N) , survey_id ], all = T)
                       
                       
                        #here's all of our effort:
                        mi_catch_eff_merge[ , .N ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]
                        #no catch data exist (or matched) for these data:
                        mi_catch_eff_merge[ is.na(species.1)  , .N ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]
                        #no_taxa_found
                        mi_catch_eff_merge[   , nothing_caught  := is.na(species.1) ,  ]
                        
                        
                        #no effort data were submitted for these fish:
                        mi_catch_eff_merge[ is.na(date.1_effort)  , .N ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]

                        
       #clean up the column names and tidy the data table up a bit                 
       
              sort(colnames(mi_catch_eff_merge))                 
              
              #county     
              mi_catch_eff_merge[ is.na(county_mergedcatch)|is.na(county_effort) , .N ,   ]
              mi_catch_eff_merge[ !(is.na(county_mergedcatch)&is.na(county_effort)) & county_mergedcatch != county_effort  , .N ,   ]# no cases where the county doesn't match
              mi_catch_eff_merge[  , .N , .(is.na(county_mergedcatch), is.na(county_effort))  ]#there are some cases where we haven't got a county at all, otherwise the effort county covers all. 
              mi_catch_eff_merge[  , county_mergedcatch := NULL]
              setnames(mi_catch_eff_merge, "county_effort", "county")
              
              #data_type
              setnames(mi_catch_eff_merge, "data_type", "data_type_effort")
                   
              #date.1
              mi_catch_eff_merge[ is.na(date.1_mergedcatch)|is.na(date.1_effort) , .N ,   ]
              mi_catch_eff_merge[ !(is.na(date.1_mergedcatch)&is.na(date.1_effort)) & date.1_mergedcatch != date.1_effort  , .N ,   ]# no cases where the dates don't match
              mi_catch_eff_merge[  , .N , .(is.na(date.1_mergedcatch), is.na(date.1_effort))  ]#there are some cases where we haven't got a date at all, otherwise the effort info covers all. 
              mi_catch_eff_merge[  , date.1_mergedcatch := NULL]
              setnames(mi_catch_eff_merge, "date.1_effort", "date.1")
              
              #date recieved
              setnames(mi_catch_eff_merge, "date_recieved", "date_recieved_effort")
              
              #effort units
              mi_catch_eff_merge[ is.na(effort_units.1_mergedcatch)|is.na(effort_units.1_effort) , .N ,   ]
              mi_catch_eff_merge[ !(is.na(effort_units.1_mergedcatch)&is.na(effort_units.1_effort)) & effort_units.1_mergedcatch != effort_units.1_effort  , .N ,   ]# no cases where the units don't match
              mi_catch_eff_merge[  , .N , .(is.na(effort_units.1_mergedcatch), is.na(effort_units.1_effort))  ]#we have efforts for all. slide into single column. 
              
              mi_catch_eff_merge[is.na(effort_units.1_effort), .N , ]
              mi_catch_eff_merge[is.na(effort_units.1_effort) , effort_units.1_effort := effort_units.1_mergedcatch , ]
              mi_catch_eff_merge[ , effort_units.1_mergedcatch := NULL , ]
               setnames(mi_catch_eff_merge, "effort_units.1_effort", "effort_units.1")
              
              #filenumber
              setnames(mi_catch_eff_merge, "file_number", "file_number_effort")
              
              #state
              mi_catch_eff_merge[ ,state := "Michigan"  ] 
              mi_catch_eff_merge[ , `:=` (state_catch = NULL, state_catchlengths = NULL)  , ]
              
              #total effort
              mi_catch_eff_merge[total_effort_1.1_effort != total_effort_1.1_mergedcatch, .N,  ]
              plot(total_effort_1.1_effort ~ total_effort_1.1_mergedcatch, data = mi_catch_eff_merge  )
              abline(1,0)
              
              #effort vals
              mi_catch_eff_merge[ total_effort_1.1_effort != total_effort_1.1_mergedcatch , .N , .(total_effort_1.1_mergedcatch, total_effort_1.1_effort)]
              #I think that the merged catch values were assigned to indiv fish (like "this fish was caught in ONE net lift") and the effort file has survey X gear total efforts
              mi_statustrends_catchlengthclass_03July2023[ , summary(total_effort_1.1) , ] 
                mi_statustrends_catchlengthclass_03July2023[total_effort_1.1 > 1 , summary(total_count), sampling_method_abbrev]
              # mi_statustrends_catch_16Mar2021[ , summary(total_effort_1.1) , ]    #this won run b/c no col for effort in that       
              mi_statustrends_effort_16Mar2021[ , summary(total_effort_1.1) ,]          
              # well--- I don't know what to make of all this, but for now I'll be keeping both of these "total_effort" variables, and leaning on the one originating in the effort file
              
              
              
              #year
              
              mi_catch_eff_merge[ year_effort != year_mergedcatch , ,]
              mi_catch_eff_merge[ , .N , .(is.na(year_effort), is.na(year_mergedcatch))]
              mi_catch_eff_merge[is.na(year_effort), year_effort := year_mergedcatch , ]
              mi_catch_eff_merge[ ,  year_mergedcatch := NULL , ]
              setnames(mi_catch_eff_merge, "year_effort", "year")
              
              
              # most of this is waste-of-time junk. Let's move the big ones left and leave this mess hang out there to the right.
              notgarbage <-  c("county","lake_id", "lake_name.1", "date.1", "year", "survey_id",  #survey 
                               "sampling_method_abbrev", "total_effort_1.1_effort", "effort_units.1", "nothing_caught",  #gear
                               "species.1", "ident", "length.1", "length_unit.1", "ident_l" #fish
                               )
              
              setcolorder(mi_catch_eff_merge, notgarbage)
               
              
        #expand these data to cover all interested species in each surveyXgear
                        
                        #check behavior now:
                        mi_catch_eff_merge[ species.1 == "WAE" , .N  , c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]
                        #we can see that to gen a catch or CPUE dataset we can cast wide (like we did in MN)
        
        #generate a species obs matrix
                        #clean dates:
                        mi_catch_eff_merge[ , unique(date.1) , ]
                        
                        #execute
                        mi_catch_eff_merge[ , date_clean := as.IDate(date.1) ,]
                        mi_catch_eff_merge[ , summary(date_clean) , ]
                        mi_catch_eff_merge[ is.na(date_clean) , .N , .(survey_id, lake_name.1)] #missing effort data here, thus the gap
                        
                        
                        #tag codes with "taxon"
                        mi_catch_eff_merge[ , species.1 := paste("taxon_",species.1, sep = "")  ,]
                        
                        #we called this "wide complete" in MN
                        wide_complete <- dcast(mi_catch_eff_merge[ ,.N , by = c("county","lake_id", "lake_name.1", "date.1", "year", "survey_id",  #survey 
                                                                           "sampling_method_abbrev", "total_effort_1.1_effort", "effort_units.1", "nothing_caught", 
                                                                           "species.1"
                                                                           )] , ... ~ species.1 , value.var = "N", fill = 0)
                        
                        wide_complete[ , c("county","lake_id", "lake_name.1", "date.1", "year", "survey_id",  #survey 
                                                                           "sampling_method_abbrev", "total_effort_1.1_effort", "effort_units.1", "taxon_WAE") , ]
                        
                        wide_complete[taxon_NA >0]
                        
                        
          # saveRDS(mi_catch_eff_merge, file = "Data_and_Scripts/Data/output/mi_flat_effort_indivfish_merge.rds")                
                        
                        
                        
                        
```                        
                        
 


# Review & QC datasets
```{r}
#here I do some very basic checks on what the data structure and general outputs look like (i.e., s this thing behaving like the obs-level file I think it is?). MI work left here as an idea of a previous state's work. In my opinion, it is not our job to QC the actual observations at this point (like, is a WAE really going to be 500mm at age zero), but instead to use this QC as a check on the operations performed in this script.  



                        #effort per surveyXgear?
                        mi_catch_eff_merge[ , .(effort = first(total_effort_1.1_effort), units = first(effort_units.1)) ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev")]
                        
                        #effort per survey type?
                        mi_catch_eff_merge[!is.na(total_effort_1.1_effort) , .(effort = first(total_effort_1.1_effort), units = first(effort_units.1), number = .N) ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev")][ ,.(effort = sum(effort), counts = sum(number), grandCPUE = sum(number)/sum(effort)) , .(sampling_method_abbrev, units)]
                        
                        #effort per survey type (Walleye ONLY)?
                        mi_catch_eff_merge[!is.na(total_effort_1.1_effort) & species.1=="walleye" , .(effort = first(total_effort_1.1_effort), units = first(effort_units.1), number = .N) ,   c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev")][ ,.(effort = sum(effort), counts = sum(number), grandCPUE = sum(number)/sum(effort)) , .(sampling_method_abbrev, units)]
                        
                        
                        
                        #how many Walleye in surveys where we had effort data?
                        mi_catch_eff_merge[ species.1== "walleye"  , .("n_fish" = .N) , .(sampling_method_abbrev) ][, sum(n_fish)]
                        mi_catch_eff_merge[ , .N , species.1]
                        
                        
                        #whats the effort look like?
                        mi_catch_eff_merge[ ,.N, total_effort_1.1_effort ]
                        
                        # data coverage
                        # how many surveys were we missing effort data for? One survey, 3 gears. Survey 4042 on Twin Lake
                        mi_catch_eff_merge[ is.na(total_effort_1.1_effort), .N , ]
                        mi_catch_eff_merge[ is.na(total_effort_1.1_effort), c("numberofspp" = length(unique(species.1))) , c("lake_name.1", "lake_id", "survey_id", "sampling_method_abbrev") ]
                        
                        
                        # how many surveys were we missing catch data from?
                        # how many surveys missing catchlength for?
                        mi_catch_eff_merge[ , .N, is.na(original_file_name.1_catch)]
                        mi_catch_eff_merge[ , .N, .(catchNA = is.na(original_file_name.1_catch),
                                                    catchlengthNA = is.na(original_file_name.1_catchlengths),
                                                    effortNA = is.na(original_file_name.1_effort))]
                        mi_catch_eff_merge[is.na(original_file_name.1_catch)]
                        
                        
                        
                        glimpse(mi_catch_eff_merge)
                        
                        
                        #Naming scheme updated to match overall approach:
                        
                      
                        

```


# Import/Export files

```{r}


#save to disk:

# saveRDS(mi_catch_eff_merge, file = "Data_and_Scripts\\Data\\output\\mi_flat_effort_indivfish_merge.rds")
# mi_catch_eff_merge <- readRDS(file = "Data_and_Scripts\\Data\\output\\mi_flat_effort_indivfish_merge.rds")


str(mi_catch_eff_merge)


mi_catch_eff_merge <- as_arrow_table(mi_catch_eff_merge)

write_dataset(dataset = mi_catch_eff_merge, path = "Data_and_Scripts/Data/output/mi_file_arrow")

mi_data <- open_dataset("Data_and_Scripts/Data/output/mi_file_arrow")

glimpse(mi_data)

```




